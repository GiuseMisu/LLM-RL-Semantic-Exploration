import os
import json
import statistics
import time
import ast
import re
import sys


sys.path.append(os.path.join(os.path.dirname(__file__), "../../../"))
from src.methods.llm_guided.llm_shared_utils import BaseLLMClient

class RobustCachedLLMClient(BaseLLMClient):
    """
    A Wrapper Client that adds caching and physics-based guardrails:
    1. Persistence: Caches rewards to disk (JSON) to ensure deterministic runs
    2. Stability: Uses Median Voting (queries LLM for N times) to ignore hallucinations
    3. Physics Guardrails: Prevents rewards for physically impossible actions (e.g., interacting from far away)
    """

    def __init__(self, real_llm_client: BaseLLMClient, cache_path="llm_reward_cache.json", voting_samples=3):
        """
        Args:
            real_llm_client: An instance of GeminiLLMClient, Phi35LLMClient, etc
            cache_path (str): File path to store the JSON cache
            voting_samples (int): How many times to query the LLM on a cache miss (3 or 5)
        """
        
        super().__init__(debug=False)
        
        self.client = real_llm_client
        self.cache_path = cache_path
        self.voting_samples = voting_samples
        
        # Load existing cache or create fresh
        self.cache = self._load_cache()
        
        # Tracking statistics
        self.stats = {
            "hits": 0,
            "misses": 0,
            "corrected_by_guardrail": 0
        }

    def _load_cache(self):
        #Loads the JSON cache from disk
        if os.path.exists(self.cache_path):
            try:
                with open(self.cache_path, 'r') as f:
                    print(f"[CACHE] Loaded existing cache from {self.cache_path}")
                    return json.load(f)
            except json.JSONDecodeError:
                print(f"[CACHE] Warning: Cache file corrupted. Starting fresh.")
                return {}
        return {}

    def _save_cache(self):
        #Saves the current cache to disk        
        try:
            with open(self.cache_path, 'w') as f:
                json.dump(self.cache, f, indent=2, sort_keys=True)
        except Exception as e:
            print(f"[CACHE] Error saving cache: {e}")

    def _apply_physics_guardrails(self, observation_str: str, proposed_reward: float) -> float:
        """
        The 'Physics' Reality Check
        Ensures the reward does not violate physical laws (e.g., grabbing items from far away)
        CRITICAL: It only enforces physics (e.g., "Cannot pick up key if dist > 1")
        """
        try:
            # Parse the observation string back into a Python Dictionary
            # The textualizer uses single quotes, so json.loads might fail. 
            # ast.literal_eval is safer for Python-dictionary-like strings.
            obs_data = ast.literal_eval(observation_str)
        except Exception as e:
            # If parsing fails, we cannot verify. Return original reward to be safe.
            # (Or log an error in debug mode)
            print(f"[GUARDRAIL] Warning: Failed to parse observation for guardrail: {e}")
            return proposed_reward

        # --- GUARDRAIL: INTERACTION DISTANCE ---
        # A reward > 0.4 implies a successful interaction (Pickup/Open) or reaching a sub-goal.
        # If the reward is high, we must verify we are physically close to *something* relevant.
        
        # Check Key Reachability
        key_info = obs_data.get('Key', '')
        # We look for the "<REACHABLE>" tag or 'dist=1' in the string generated by New_textualizer
        key_is_reachable = "<REACHABLE>" in key_info

        # Check Door Reachability
        door_info = obs_data.get('Door', '')
        door_is_reachable = "<REACHABLE>" in door_info

        # LOGIC:
        # If the LLM gives a high reward 
        # but the object is NOT reachable, it is a hallucination (in system prompt there is the reward guideline)       
        # Note: careful not to penalize wandering rewards (0.1)
        # punish false high rewards obtained without true interaction
        
        if proposed_reward >= 0.5:
            # If we are being rewarded for Key (usually 0.5 is key found/retrieved)
            # *some* interaction condition must be met
            
            # If NOTHING is reachable, a reward >= 0.5 is physically impossible 
            # unless it's the final Goal
            if not key_is_reachable and not door_is_reachable:
                # Exception: Maybe we are AT the goal? 
                goal_info = obs_data.get('Goal', '')
                if "<REACHABLE>" not in goal_info:
                    # VALIDATION FAILED: The agent is hallucinating success from afar
                    self.stats["corrected_by_guardrail"] += 1
                    return 0.1 # Downgrade to wandering score
        
        return proposed_reward

    def _get_raw_response(self, prompt: str, generate_explanation: bool) -> str:
        #Required implementation of abstract method.
        #Since this is a wrapper, delegate to the real client.
        return self.client._get_raw_response(prompt, generate_explanation)

    def get_reward(self, observation: str, verbose: bool = False, generate_explanation: bool = False) -> float:
        #The main public method called by the RL agent.
        
        # 1. Normalize the Key
        # Strip whitespace to ensure matches
        cache_key = observation.strip()

        # 2. Check Cache (Hit)
        if cache_key in self.cache:
            self.stats["hits"] += 1
            if verbose:
                print(f"[CACHE HIT] Reward: {self.cache[cache_key]}")
            return float(self.cache[cache_key])

        # 3. Cache Miss (Logic)
        self.stats["misses"] += 1
        if verbose:
            print(f"[CACHE MISS] querying LLM {self.voting_samples} times")

        rewards = []
        
        # --- VOTING LOOP ---
        for i in range(self.voting_samples):
            # We call the real client
            # this triples the time for the first encounter of a state
            r = self.client.get_reward(cache_key, verbose=False, generate_explanation=generate_explanation)
            rewards.append(r)
        
        # 4. Calculate Median = robust against outliers 
        final_reward = statistics.median(rewards)
        
        if verbose:
            print(f"   -> Raw Votes: {rewards}")
            print(f"   -> Median: {final_reward}")

        # 5. Apply Physics Guardrails
        # Verify the reward doesn't break reality
        guarded_reward = self._apply_physics_guardrails(cache_key, final_reward)
        
        if guarded_reward != final_reward and verbose:
            print(f"   -> [GUARDRAIL] Corrected hallucination: pre-guard:{final_reward} => guard:{guarded_reward}")

        # 6. Save to Cache
        self.cache[cache_key] = guarded_reward
        self._save_cache()

        return guarded_reward



if __name__ == "__main__":
    from src.methods.llm_guided.phi3_5 import Phi35LLMClient
    
    # 1. Initialize the Real Client
    try:
        real_client = Phi35LLMClient(debug=True)
        print(f"Client Initialized Model: {real_client.model_name}")
    except Exception as e:
        print(e)
    # 2. Wrap it to get the cacged and guardrail version
    cache_name = "test_PHI_cache.json"
    cached_client = RobustCachedLLMClient(real_client, cache_path=cache_name, voting_samples=3)
   
    print("\n--- TEST 1: Reachable Key (Expect Valid Reward) ---")
    # Case A: Reachable Key (Should keep high reward)
    obs_reachable = "{ 'Agent': { 'pos': (1, 1) }, 'Key': 'loc=(2, 1), dist=1, dir=Front <REACHABLE>', 'Door': 'Not Found' }"
    r1 = cached_client.get_reward(obs_reachable, verbose=True)
    print(f"Final Reward 1: {r1}")

    print("\n--- TEST 2: Far Away (Checking Guardrail) ---")
    # Case B: Unreachable Key (Should downgrade high reward if hallucinated)
    obs_far_away = "{ 'Agent': { 'pos': (1, 1) }, 'Key': 'loc=(5, 5), dist=8, dir=South', 'Door': 'Not Found' }"
    #without api call send directly a wrong high reward
    corrected = cached_client._apply_physics_guardrails(obs_far_away, 0.5)
    print(f"Input Reward: 0.5 -> Guardrail Output: {corrected}")
    
    print("\n--- TEST 3: Cache Persistence & Speed ---")
    obs_repeat = "{ 'Agent': { 'pos': (2, 2) }, 'Key': 'Not Found', 'Door': 'Not Found' }"
    
    print("First Call (Should hit API)")
    r_first = cached_client.get_reward(obs_repeat, verbose=False)
    print(f"   -> Reward: {r_first}")

    print("Second Call (Should hit Cache)")
    r_second = cached_client.get_reward(obs_repeat, verbose=False)
    print(f"   -> Reward: {r_second}")

    print("\n--- TEST 4: Edge Cases & Door Logic ---")
    
    # CASE A: Hallucinating the DOOR
    # The agent is far from the door (dist=5), but LLM gives 0.8 reward.
    # Guardrail should catch this.
    obs_door_hallucination = "{ 'Agent': { 'pos': (1, 1) }, 'Key': 'Held', 'Door': 'loc=(6, 6), dist=5, state=locked', 'Goal': 'Unknown' }"
    res_door = cached_client._apply_physics_guardrails(obs_door_hallucination, 0.8)
    
    # CASE B: Valid Low Reward
    # The agent is far from everything. LLM correctly gives 0.05.
    # Guardrail should NOT change this (it should not force it to 0.1 if it's already lower/correct).
    obs_wandering = "{ 'Agent': { 'pos': (1, 1) }, 'Key': 'dist=5', 'Door': 'Unknown' }"
    res_wandering = cached_client._apply_physics_guardrails(obs_wandering, 0.05)

    print(f"Case A (Door Hallucination): Input 0.8 -> Output {res_door}")
    print(f"Case B (Valid Low Reward):   Input 0.05 -> Output {res_wandering}")
