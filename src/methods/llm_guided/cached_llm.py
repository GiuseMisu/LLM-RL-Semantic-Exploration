import os
import json
import statistics
import time
import ast
import re
import sys


sys.path.append(os.path.join(os.path.dirname(__file__), "../../../"))
from src.methods.llm_guided.llm_shared_utils import BaseLLMClient

class RobustCachedLLMClient(BaseLLMClient):
    """
    A Wrapper Client that adds caching and physics-based guardrails:
    1. Persistence: Caches rewards to disk (JSON) to ensure deterministic runs
    2. Stability: Uses Median Voting (queries LLM for N times) to ignore hallucinations
    3. Physics Guardrails: Prevents rewards for physically impossible actions (e.g., interacting from far away)
    """

    def __init__(self, real_llm_client: BaseLLMClient, cache_path="llm_reward_cache.json", voting_samples=3):
        """
        Args:
            real_llm_client: An instance of GeminiLLMClient, Phi35LLMClient, etc
            cache_path (str): File path to store the JSON cache
            voting_samples (int): How many times to query the LLM on a cache miss (3 or 5)
        """
        
        super().__init__(debug=False)
        
        self.client = real_llm_client
        self.cache_path = cache_path
        self.voting_samples = voting_samples
        
        # Load existing cache or create fresh
        self.cache = self._load_cache()
        
        # Tracking statistics
        self.stats = {
            "hits": 0,
            "misses": 0,
            "corrected_by_guardrail": 0
        }

    def _load_cache(self):
        #Loads the JSON cache from disk
        if os.path.exists(self.cache_path):
            try:
                with open(self.cache_path, 'r') as f:
                    print(f"[CACHE] Loaded existing cache from {self.cache_path}")
                    return json.load(f)
            except json.JSONDecodeError:
                print(f"[CACHE] Warning: Cache file corrupted. Starting fresh.")
                return {}
        return {}

    def _save_cache(self):
        #Saves the current cache to disk        
        try:
            with open(self.cache_path, 'w') as f:
                json.dump(self.cache, f, indent=2, sort_keys=True)
        except Exception as e:
            print(f"[CACHE] Error saving cache: {e}")

    def _apply_physics_guardrails(self, observation_str: str, proposed_reward: float) -> float:
        """
        The 'Physics' Reality Check
        Ensures the reward does not violate physical laws (e.g., grabbing items from far away)
        CRITICAL: It only enforces physics (e.g., "Cannot pick up key if dist > 1")
        """
        try:
            # Parse the observation string back into a Python Dictionary
            # The textualizer uses single quotes, so json.loads might fail. 
            # ast.literal_eval is safer for Python-dictionary-like strings.
            obs_data = ast.literal_eval(observation_str)
        except Exception as e:
            # If parsing fails, we cannot verify. Return original reward to be safe.
            # (Or log an error in debug mode)
            print(f"[GUARDRAIL] Warning: Failed to parse observation for guardrail: {e}")
            return proposed_reward

        #==============old guardrail================
        # # --- GUARDRAIL: INTERACTION DISTANCE ---
        # # A reward > 0.4 implies a successful interaction (Pickup/Open) or reaching a sub-goal.
        # # If the reward is high, we must verify we are physically close to *something* relevant.
        # # Check Key Reachability
        # key_info = obs_data.get('Key', '')
        # # We look for the "<REACHABLE>" tag or 'dist=1' in the string generated by New_textualizer
        # key_is_reachable = "<REACHABLE>" in key_info
        # # Check Door Reachability
        # door_info = obs_data.get('Door', '')
        # door_is_reachable = "<REACHABLE>" in door_info

        # # LOGIC:
        # # If the LLM gives a high reward 
        # # but the object is NOT reachable, it is a hallucination (in system prompt there is the reward guideline)       
        # # Note: careful not to penalize wandering rewards (0.1)
        # # punish false high rewards obtained without true interaction
        # if proposed_reward >= 0.5:
        #     # If we are being rewarded for Key (usually 0.5 is key found/retrieved)
        #     # *some* interaction condition must be met            
        #     # If NOTHING is reachable, a reward >= 0.5 is physically impossible 
        #     # unless it's the final Goal
        #     if not key_is_reachable and not door_is_reachable:
        #         # Exception: Maybe we are AT the goal? 
        #         goal_info = obs_data.get('Goal', '')
        #         if "<REACHABLE>" not in goal_info:
        #             # VALIDATION FAILED: The agent is hallucinating success from afar
        #             self.stats["corrected_by_guardrail"] += 1
        #             return 0.1 # Downgrade to wandering score
        # return proposed_reward

        # --- NEW GUARDRAIL ---
        #----is the goal guardrail too much??----
        # --- GUARDRAIL: INSTANT WIN OVERRIDE ---
        # If the agent is literally ON the goal, the LLM's opinion doesn't matter.
        # It is a win. Force 1.0.
        # goal_info = obs_data.get('Goal', '')
        # if "dist=0" in goal_info or "dir=Here" in goal_info:
        #     if proposed_reward < 1.0:
        #         # print("[GUARDRAIL] Agent is on Goal. Forcing reward to 1.0.") # Optional logging
        #         return 1.0
        #     return proposed_reward
        
        if proposed_reward >= 0.5:
            # Check Key Reachability
            key_info = obs_data.get('Key', '')
            key_is_reachable = "<REACHABLE>" in key_info or "In Inventory" in key_info

            # Check Door Reachability
            door_info = obs_data.get('Door', '')
            door_is_reachable = "<REACHABLE>" in door_info

            # Check Goal Reachability
            goal_info = obs_data.get('Goal', '')
            # We are at the goal if valid reachability tag OR we are standing on it (dist=0/Here)
            goal_is_reachable = "<REACHABLE>" in goal_info or "dist=0" in goal_info or "dir=Here" in goal_info

            # LOGIC:
            # If we are strictly seeking the Key/Door (Phase 1/2), ensure we are close.
            # If we are at the Goal (Phase 3), that overrides everything.
            
            if not key_is_reachable and not door_is_reachable and not goal_is_reachable:
                # VALIDATION FAILED: The agent is hallucinating success from afar
                self.stats["corrected_by_guardrail"] += 1
                return 0.1 
        
        return proposed_reward

    def _get_raw_response(self, prompt: str, generate_explanation: bool) -> str:
        #Required implementation of abstract method.
        #Since this is a wrapper, delegate to the real client.
        return self.client._get_raw_response(prompt, generate_explanation)

    def robust_get_reward(self, observation: str, verbose: bool = False, generate_explanation: bool = False) -> float:
        #The main public method called by the RL agent.
        
        # 1. Normalize the Key
        # Strip whitespace to ensure matches
        cache_key = observation.strip()

        # 2. Check Cache (Hit)
        if cache_key in self.cache:
            self.stats["hits"] += 1
            if verbose:
                print(f"[CACHE HIT] Reward: {self.cache[cache_key]}")
            return float(self.cache[cache_key])

        # 3. Cache Miss (Logic)
        self.stats["misses"] += 1
        if verbose:
            print(f"[CACHE MISS] querying LLM {self.voting_samples} times")

        rewards = []
        
        # --- VOTING LOOP ---
        for i in range(self.voting_samples):
            # We call the real client
            # this triples the time for the first encounter of a state
            r = self.client.get_reward(cache_key, verbose=verbose, generate_explanation=generate_explanation)
            rewards.append(r)
        
        # 4. Calculate Median = robust against outliers 
        final_reward = statistics.median(rewards)
        
        if verbose:
            print(f"   -> Raw Votes: {rewards}")
            print(f"   -> Median: {final_reward}")

        # 5. Apply Physics Guardrails
        # Verify the reward doesn't break reality
        guarded_reward = self._apply_physics_guardrails(cache_key, final_reward)
        
        if guarded_reward != final_reward and verbose:
            print(f"   -> [GUARDRAIL] Corrected hallucination: pre-guard:{final_reward} => guard:{guarded_reward}")

        # 6. Save to Cache
        self.cache[cache_key] = guarded_reward
        self._save_cache()

        return guarded_reward



if __name__ == "__main__":
    from src.methods.llm_guided.phi3_5 import Phi35LLMClient
    
    # 1. Initialize the Real Client
    try:
        real_client = Phi35LLMClient(debug=True)
        print(f"Client Initialized Model: {real_client.model_name}")
    except Exception as e:
        print(e)
    # 2. Wrap it to get the cacged and guardrail version
    cache_name = "test_PHI_cache.json"
    cached_client = RobustCachedLLMClient(real_client, cache_path=cache_name, voting_samples=3)
   
    # print("\n--- TEST 1: Reachable Key (Expect Valid Reward) ---")
    # # Case A: Reachable Key (Should keep high reward)
    # obs_reachable = "{ 'Agent': { 'pos': (1, 1) }, 'Key': 'loc=(2, 1), dist=1, dir=Front <REACHABLE>', 'Door': 'Not Found' }"
    # r1 = cached_client.robust_get_reward(obs_reachable, verbose=True)
    # print(f"Final Reward 1: {r1}")

    # print("\n--- TEST 2: Far Away (Checking Guardrail) ---")
    # # Case B: Unreachable Key (Should downgrade high reward if hallucinated)
    # obs_far_away = "{ 'Agent': { 'pos': (1, 1) }, 'Key': 'loc=(5, 5), dist=8, dir=South', 'Door': 'Not Found' }"
    # #without api call send directly a wrong high reward
    # corrected = cached_client._apply_physics_guardrails(obs_far_away, 0.5)
    # print(f"Input Reward: 0.5 -> Guardrail Output: {corrected}")
    
    # print("\n--- TEST 3: Cache Persistence & Speed ---")
    # obs_repeat = "{ 'Agent': { 'pos': (2, 2) }, 'Key': 'Not Found', 'Door': 'Not Found' }"
    
    # print("First Call (Should hit API)")
    # r_first = cached_client.robust_get_reward(obs_repeat, verbose=False)
    # print(f"   -> Reward: {r_first}")

    # print("Second Call (Should hit Cache)")
    # r_second = cached_client.robust_get_reward(obs_repeat, verbose=False)
    # print(f"   -> Reward: {r_second}")

    # print("\n--- TEST 4: Edge Cases & Door Logic ---")
    
    # # CASE A: Hallucinating the DOOR
    # # The agent is far from the door (dist=5), but LLM gives 0.8 reward.
    # # Guardrail should catch this.
    # obs_door_hallucination = "{ 'Agent': { 'pos': (1, 1) }, 'Key': 'Held', 'Door': 'loc=(6, 6), dist=5, state=locked', 'Goal': 'Unknown' }"
    # res_door = cached_client._apply_physics_guardrails(obs_door_hallucination, 0.8)
    
    # # CASE B: Valid Low Reward
    # # The agent is far from everything. LLM correctly gives 0.05.
    # # Guardrail should NOT change this (it should not force it to 0.1 if it's already lower/correct).
    # obs_wandering = "{ 'Agent': { 'pos': (1, 1) }, 'Key': 'dist=5', 'Door': 'Unknown' }"
    # res_wandering = cached_client._apply_physics_guardrails(obs_wandering, 0.05)

    # print(f"Case A (Door Hallucination): Input 0.8 -> Output {res_door}")
    # print(f"Case B (Valid Low Reward):   Input 0.05 -> Output {res_wandering}")

    # CASE 5: Phase 3 Transition - Door is Open
    # Scenario: Door is Open. Agent is walking through.
    # Logic: Door state='Open' -> Phase 2 Complete. Reward should be 0.7.
    # obs_5 = "{ 'Agent': { 'pos': (2, 2), 'facing': 'East', 'inventory': 'Key' }, 'Key': 'loc=None', 'Door': 'loc=(2, 2), dist=0, dir=Here, state=Open', 'Goal': 'loc=(3, 3), dist=2, dir=Front-Right' }"
    # res_obs5 = cached_client.robust_get_reward(obs_5, verbose=True)

    # case 6
    # holding the key and standing right in front of an Open door, should get 0.7 reward
    # obs_6 = "{ 'Agent': { 'pos': (3, 3), 'facing': 'East', 'inventory': 'Yellow Key' }, 'Key': 'In Inventory (Carried)', 'Door': 'loc=(4, 3), dist=1, dir=Front <REACHABLE>, state=Open', 'Goal': 'loc=(6, 6), dist=5, dir=Right' }"
    # res_obs6 = cached_client.robust_get_reward(obs_6, verbose=True)

    # CASE 7: Holding Key but Door is Unlocked (not Open)
    # holding the key and facing a door that is Unlocked (but technically closed)
    # obs_7 = "{ 'Agent': { 'pos': (2, 5), 'facing': 'North', 'inventory': 'Blue Key' }, 'Key': 'In Inventory (Carried)', 'Door': 'loc=(2, 4), dist=1, dir=Front <REACHABLE>, state=Unlocked', 'Goal': 'loc=(2, 1), dist=4, dir=Front' }"
    # res_obs7 = cached_client.robust_get_reward(obs_7, verbose=True)

    # CASE 7: Goal Reached
    # Scenario: Agent overlaps with Goal.
    # Logic: Dist to Goal is 0. Reward should be 1.0.
    obs_goal = "{ 'Agent': { 'pos': (3, 3), 'facing': 'South', 'inventory': 'Yellow Key' }, 'Key': 'In Inventory (Carried)', 'Door': 'loc=(2, 2), dist=2, dir=Behind, state=Open', 'Goal': 'loc=(3, 3), dist=0, dir=Here' }"
    res_goal = cached_client.robust_get_reward(obs_goal, verbose=True)

    obs_goal_2 = "{ 'Agent': { 'pos': (3, 2), 'facing': 'South', 'inventory': 'Yellow Key' }, 'Key': 'In Inventory (Carried)', 'Door': 'loc=(2, 2), dist=1, dir=Right, state=Open', 'Goal': 'loc=(3, 3), dist=1, dir=Front <REACHABLE>' }"
    res_goal_2 = cached_client.robust_get_reward(obs_goal_2, verbose=True) 